{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNxgbEfHPvm7"
   },
   "source": [
    "## Lab 10: Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZgIwOTDPvm_"
   },
   "source": [
    "## Learning Goals\n",
    "\n",
    "In this lab we will look at Recurrent Neural Networks (RNNs), LSTMs and their building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h0r2RYXPvm_"
   },
   "source": [
    "In this task, we are going to do sentiment classification on a movie review dataset. We are going to build a feedforward net, a convolutional neural net, a recurrent net and combine one or more of them to understand performance of each of them. A sentence can be thought of as a sequence of words that collectively represent meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Downloading h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Downloading keras-3.2.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.3.5)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Using cached namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow)\n",
      "  Using cached optree-0.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (45 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Using cached tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl (227.0 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl (10.0 MB)\n",
      "Downloading h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.2.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl (26.4 MB)\n",
      "Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl (389 kB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl (3.4 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.11.0-cp311-cp311-macosx_11_0_arm64.whl (274 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.11.0 keras-3.2.1 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.7 opt-einsum-3.3.0 optree-0.11.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ho6zSv6oPvnA"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Embedding, Bidirectional\n",
    "import numpy as np\n",
    "import keras\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UoDYTB_zPvnA"
   },
   "outputs": [],
   "source": [
    "# We want to have a finite vocabulary to make sure that our word matrices are not arbitrarily small\n",
    "vocabulary_size = 10000\n",
    "\n",
    "# We also want to have a finite length of reviews and not have to process really long sentences.\n",
    "max_review_length = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjh5w_NVPvnB"
   },
   "source": [
    "Computers have no built-in knowledge of words or their meanings and cannot understand them in any rich way that humans do -- hence, the purpose of Natural Language Processing (NLP). As with any data science, computer science, machine learning task, the first crucial step is to clean (pre-process) your data so that you can soundly make use of it. Within NLP, this first step is called Tokenization and it concerns how to represent each token (a.k.a. word) of your corpus (i.e., dataset).\n",
    "\n",
    "#### TOKENIZATION\n",
    "\n",
    "A ``token`` refers to a single, atomic unit of meaning (i.e., a word). How should our computers represent each word? We could read in our corpus word by word and store each word as a String (data structure). However, Strings tend to use more computer memory than Integers and can become cumbersome. As long as we preserve the uniqueness of the tokens and are consistent, we are better off converting each distinct word to a distinct number (Integer). This is standard practice within NLP / computer science / data science, etc.\n",
    "As a simple example of tokenization, we can see a small example.\n",
    "\n",
    "If the five sentences below were our entire corpus, our conversion would look as follows:\n",
    "\n",
    "1. i have books - [1, 4, 7]\n",
    "2. interesting books are useful [10,2,9,8]\n",
    "3. i have computers [1,4,6]\n",
    "4. computers are interesting and useful [6,9,11,10,8]\n",
    "5. books and computers are both valuable. [2,10,2,9,13,12]\n",
    "6. Bye Bye [7,7]\n",
    "\n",
    "Create tokens for vocabulary based on frequency of occurrence. Hence, we assign the following tokens\n",
    "\n",
    "I-1, books-2, computers-3, have-4, are-5, computers-6,bye-7, useful-8, are-9, and-10,interesting-11, valuable-12, both-13\n",
    "\n",
    "Thankfully, our dataset is already represented in such a tokenized form.\n",
    "\n",
    "**NOTE:** Often times, depending on your NLP task, it is useful to also perform other pre-processing, cleaning steps, such as:\n",
    "- treating each punctuation mark as a token (e.g., , . ! ? are each separate tokens)\n",
    "- lower-casing all words (so that a given word isn't treated differently just because it starts a sentence or not)\n",
    "- separating each sentence with a unique symbol (e.g., <S> and </S>)\n",
    "- removing words that are incredibly common (e.g., function words, (in)definite articles). These are referred to as 'stopwords'). For language modelling, we DO NOT remove stopwords. A sentence's meaning needs to include all of the original words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFaIXdz4PvnB"
   },
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRW7KpICPvnB",
    "outputId": "5aeac6b8-7730-4d99-bb0c-bb550fff7e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews 25000\n",
      "Length of first and fifth review before padding 218 147\n",
      "First review [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "First label 1\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
    "print('Number of reviews', len(X_train))\n",
    "print('Length of first and fifth review before padding', len(X_train[0]) ,len(X_train[4]))\n",
    "print('First review', X_train[0])\n",
    "print('First label', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86ML807KPvnC"
   },
   "source": [
    "#### Preprocess data\n",
    "\n",
    "If we were training our RNN one sentence at a time, it would be okay to have sentences of varying lengths. However, as with any neural network, it can be sometimes be advantageous to train inputs in batches. When doing so with RNNs, our input tensors need to be of the same length/dimensions. Thus, let's pad our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AligcS3ePvnC",
    "outputId": "5356fb1a-f104-4750-c761-54259d6dd798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of first and fifth review after padding 500 500\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test  = pad_sequences(X_test, maxlen=max_review_length)\n",
    "print('Length of first and fifth review after padding', len(X_train[0]) ,len(X_train[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chDrzTrXPvnC"
   },
   "source": [
    "### MODEL 1A : FEED-FORWARD NETWORKS WITHOUT EMBEDDINGS\n",
    "\n",
    "Let us build a single-layer feed-forward net with a hidden layer of 250 nodes. Each input would be a 500-dim vector of tokens since we padded all our sequences to size 500.\n",
    "\n",
    "\n",
    "<b>EXERCISE</b>: Calculate the number of parameters involved in this network and implement a feedforward net to do classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3sCiuZz9hFN"
   },
   "source": [
    "**Notes**:\n",
    "> - The number of parameters in the model is the number of weights + the number of biases.\n",
    "- The number of weights is the number of inputs multiplied by the number of outputs.\n",
    "- The number of biases is the number of outputs.\n",
    "\n",
    "> - We have 500 inputs and 250 outputs, so the number of weights is 500 * 250 = 125,000.\n",
    "- The number of biases is 250.\n",
    "> - Therefore, the total number of parameters is 125,000 + 250 = 125,250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfBotztSPvnD",
    "outputId": "67d3748b-0a62-4c25-92b1-d0518e870893"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"FEED_FORWARD_NET1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"FEED_FORWARD_NET1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">125,250</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">251</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)            │       \u001b[38;5;34m125,250\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m251\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,501</span> (490.24 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m125,501\u001b[0m (490.24 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,501</span> (490.24 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m125,501\u001b[0m (490.24 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5034 - loss: 221.1394\n",
      "Epoch 2/3\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5863 - loss: 22.5628\n",
      "Epoch 3/3\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6186 - loss: 4.1783\n",
      "Accuracy: 50.08%\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "def feedforward_net():\n",
    "  model = Sequential(name=\"FEED_FORWARD_NET1\")\n",
    "  model.add(Dense(250, input_shape=(500,), activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "# Build the model\n",
    "model = feedforward_net()\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "# Evaluate and score\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ji22jqU-er5"
   },
   "source": [
    "<b>Discussion:</b> Why was the performance bad? What was wrong with tokenization?\n",
    "> We think that the embeded forme of the word used do not take into account the similarity in between the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Blnq5PmyPvnD"
   },
   "source": [
    "### MODEL 1B : FEED-FORWARD NETWORKS WITH EMBEDDINGS\n",
    "\n",
    "#### What is an embedding layer ?\n",
    "\n",
    "An embedding is a \"distributed representation\" (e.g., vector) of a particular atomic item (e.g., word token, object, etc). When representing items by embeddings:\n",
    "- each distinct item should be represented by its own unique embedding\n",
    "- the semantic similarity between items should correspond to the similarity between their respective embeddings (i.e., words that are more similar to one another should have embeddings that are more similar to each other).\n",
    "\n",
    "There are essentially an infinite number of ways to create such embeddings, and since these representations have such a great influence on the performance of our models, there has been an incredible amount of research dedicated to this very aspect. If you are interested in learning more, start with the astromonically impactful papers of [word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) and [GloVe](https://www.aclweb.org/anthology/D14-1162.pdf).\n",
    "\n",
    "In general, though, one can view the embedding process as a linear projection from one vector space to another (e.g., a vector space of unique words being mapped to a world of fixed-length, dense vectors filled with continuous-valued numbers. For NLP, we usually use embeddings to project the one-hot encodings of words (i.e., a vector that is the length of the entire vocabulary, and it is filled with all zeros except for a single value of 1 that corresponds to the particular word) on to a lower-dimensional continuous space (e.g., vectors of size 100) so that the input surface is dense and possibly smooth. Thus, one can view this embedding layer process as just a transformation from $\\mathbb{R}^{inp}$ to $\\mathbb{R}^{emb}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sR4b-ddpPvnD"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCKVG0w4PvnD",
    "outputId": "ceb79b50-0c38-47b7-df79-e577e306013c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 100)          1000000   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50000)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 250)               12500250  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13500501 (51.50 MB)\n",
      "Trainable params: 13500501 (51.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# inputs will be converted from batch_size * sentence_length to batch_size*sentence_length*embedding _dim\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rxx_c1QyPvnE",
    "outputId": "185dde0f-ea36-482c-bc82-f91cddc622aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 - 121s - loss: 0.4699 - accuracy: 0.7604 - val_loss: 0.2986 - val_accuracy: 0.8714 - 121s/epoch - 618ms/step\n",
      "Epoch 2/2\n",
      "196/196 - 135s - loss: 0.1261 - accuracy: 0.9546 - val_loss: 0.3112 - val_accuracy: 0.8712 - 135s/epoch - 687ms/step\n",
      "Accuracy: 87.12%\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHs3RdR9PvnE"
   },
   "source": [
    "### MODEL 2 : CONVOLUTIONAL NEURAL NETWORKS (CNN)\n",
    "Text can be thought of as 1-dimensional sequence (a single, long vector) and we can apply 1D Convolutions over a set of word embeddings.\n",
    "\n",
    "<b>EXERCISE</b>: Fit a 1D convolution with 200 filters, kernel size 3, followed by a feed-forward layer of 250 nodes, and ReLU and Sigmoid activations as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLqhBSQxH7It",
    "outputId": "d05c2cb4-e3dd-4b9b-8af8-75babaa9d288"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "\n",
    "# inputs will be converted from batch_size * sentence_length to batch_size*sentence_length*embedding _dim\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(Conv1D(200, 3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai6_NLHlPvnE"
   },
   "source": [
    "### MODEL 3 : Simple RNN\n",
    "\n",
    "At a high-level, an RNN is similar to a feed-forward neural network (FFNN) in that there is an input layer, a hidden layer, and an output layer. The input layer is fully connected to the hidden layer, and the hidden layer is fully connected to the output layer. However, the crux of what makes it a **recurrent** neural network is that the hidden layer for a given time _t_ is not only based on the input layer at time _t_ but also the hidden layer from time _t-1_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbdHnegNPvnE",
    "outputId": "aaaea475-e521-4715-a929-8abff6e7be24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 100)          1000000   \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,020,201\n",
      "Trainable params: 1,020,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 104s 66ms/step - loss: 0.5913 - accuracy: 0.6777\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 0.5341 - accuracy: 0.7192\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.5670 - accuracy: 0.6981\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.4583 - accuracy: 0.7859\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 101s 64ms/step - loss: 0.3828 - accuracy: 0.8364\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 102s 65ms/step - loss: 0.3386 - accuracy: 0.8598\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.3539 - accuracy: 0.8438\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 102s 65ms/step - loss: 0.4318 - accuracy: 0.7935\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.2708 - accuracy: 0.8924\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.2409 - accuracy: 0.9062\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.2707 - accuracy: 0.8890\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.3584 - accuracy: 0.8436\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.3567 - accuracy: 0.8477\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 101s 64ms/step - loss: 0.4833 - accuracy: 0.7585\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 101s 64ms/step - loss: 0.4806 - accuracy: 0.7534\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 101s 64ms/step - loss: 0.4581 - accuracy: 0.7756\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.4499 - accuracy: 0.7787\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 102s 65ms/step - loss: 0.4761 - accuracy: 0.7560\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.4364 - accuracy: 0.8022\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.4222 - accuracy: 0.8113\n",
      "Accuracy: 69.32%\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(SimpleRNN(100))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=16)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjhG9udiPvnF"
   },
   "source": [
    "### MODEL 4 : LSTM\n",
    "\n",
    "Now, let's use an LSTM model to do classification! To make it a fair comparison to the SimpleRNN, let's start with the same architecture hyper-parameters (e.g., number of hidden nodes, epochs, and batch size). Then, experiment with increasing the number of nodes, stacking multiple layers, applying dropouts etc. Check the number of parameters that this model entails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "xkSXYtUcPvnF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 500, 300)          3000000   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               160400    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,160,501\n",
      "Trainable params: 3,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 01:11:44.199581: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-05 01:11:44.200394: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-05 01:11:44.201176: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-05 01:11:44.349459: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-05 01:11:44.350136: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-05 01:11:44.350872: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-05 01:11:44.749954: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-05 01:11:44.750918: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-05 01:11:44.751492: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 108s 274ms/step - loss: 0.4218 - accuracy: 0.8076\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 108s 275ms/step - loss: 0.2552 - accuracy: 0.9006\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 108s 276ms/step - loss: 0.2072 - accuracy: 0.9224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 01:17:07.826974: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-05 01:17:07.827710: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-05 01:17:07.828603: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.94%\n"
     ]
    }
   ],
   "source": [
    "# %load sol4.py\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "# evaluation of the LSTM's performance\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qst0NxurPvnF"
   },
   "source": [
    "### MODEL 5 : CNN + LSTM\n",
    "\n",
    "CNNs are good at learning spatial features, and sentences can be thought of as 1-D spatial vectors (dimensionality is determined by the number of words in the sentence). We apply an LSTM over the features learned by the CNN (after a maxpooling layer). This leverages the power of CNNs and LSTMs combined! We expect the CNN to be able to pick out invariant features across the 1-D spatial structure (i.e., sentence) that characterize good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer, and the final classification can be made via a feed-forward connection to a single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8yF_UYSxPvnF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 500, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 32)           9632      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 250, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,062,933\n",
      "Trainable params: 1,062,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 15:21:58.133256: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-04 15:21:58.134376: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-04 15:21:58.135319: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-04 15:21:58.369305: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-04 15:21:58.370620: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-04 15:21:58.371588: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-04 15:21:58.872299: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-04 15:21:58.873801: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-04 15:21:58.874721: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 59s 148ms/step - loss: 0.3816 - accuracy: 0.8196\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 57s 147ms/step - loss: 0.1974 - accuracy: 0.9262\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.1429 - accuracy: 0.9492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 15:26:09.556178: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-04 15:26:09.558067: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-04 15:26:09.559454: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.67%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I89L2ygHPvnG"
   },
   "source": [
    "### To do!\n",
    "\n",
    "Stard from model 5 and do the folowwing:\n",
    "\n",
    "1.   Replace keras embedding with GloVe pre-trained word embedding to CNN.\n",
    "2.   Replace the simple LSTM layer with Bidirectional LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Replace keras embedding with GloVe pre-trained word embedding to CNN.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace glove.840B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "# !wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# !unzip -q glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195884 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------------------------\n",
    "# Let's make a dict mapping words (strings) to their NumPy vector representation:\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "path_to_glove_file = \"glove.840B.300d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'a': 3,\n",
       " 'of': 4,\n",
       " 'to': 5,\n",
       " 'is': 6,\n",
       " 'br': 7,\n",
       " 'in': 8,\n",
       " 'it': 9,\n",
       " 'i': 10,\n",
       " 'this': 11,\n",
       " 'that': 12,\n",
       " 'was': 13,\n",
       " 'as': 14,\n",
       " 'for': 15,\n",
       " 'with': 16,\n",
       " 'movie': 17,\n",
       " 'but': 18,\n",
       " 'film': 19,\n",
       " 'on': 20,\n",
       " 'not': 21,\n",
       " 'you': 22,\n",
       " 'are': 23,\n",
       " 'his': 24,\n",
       " 'have': 25,\n",
       " 'he': 26,\n",
       " 'be': 27,\n",
       " 'one': 28,\n",
       " 'all': 29,\n",
       " 'at': 30,\n",
       " 'by': 31,\n",
       " 'an': 32,\n",
       " 'they': 33,\n",
       " 'who': 34,\n",
       " 'so': 35,\n",
       " 'from': 36,\n",
       " 'like': 37,\n",
       " 'her': 38,\n",
       " 'or': 39,\n",
       " 'just': 40,\n",
       " 'about': 41,\n",
       " \"it's\": 42,\n",
       " 'out': 43,\n",
       " 'has': 44,\n",
       " 'if': 45,\n",
       " 'some': 46,\n",
       " 'there': 47,\n",
       " 'what': 48,\n",
       " 'good': 49,\n",
       " 'more': 50,\n",
       " 'when': 51,\n",
       " 'very': 52,\n",
       " 'up': 53,\n",
       " 'no': 54,\n",
       " 'time': 55,\n",
       " 'she': 56,\n",
       " 'even': 57,\n",
       " 'my': 58,\n",
       " 'would': 59,\n",
       " 'which': 60,\n",
       " 'only': 61,\n",
       " 'story': 62,\n",
       " 'really': 63,\n",
       " 'see': 64,\n",
       " 'their': 65,\n",
       " 'had': 66,\n",
       " 'can': 67,\n",
       " 'were': 68,\n",
       " 'me': 69,\n",
       " 'well': 70,\n",
       " 'than': 71,\n",
       " 'we': 72,\n",
       " 'much': 73,\n",
       " 'been': 74,\n",
       " 'bad': 75,\n",
       " 'get': 76,\n",
       " 'will': 77,\n",
       " 'do': 78,\n",
       " 'also': 79,\n",
       " 'into': 80,\n",
       " 'people': 81,\n",
       " 'other': 82,\n",
       " 'first': 83,\n",
       " 'great': 84,\n",
       " 'because': 85,\n",
       " 'how': 86,\n",
       " 'him': 87,\n",
       " 'most': 88,\n",
       " \"don't\": 89,\n",
       " 'made': 90,\n",
       " 'its': 91,\n",
       " 'then': 92,\n",
       " 'way': 93,\n",
       " 'make': 94,\n",
       " 'them': 95,\n",
       " 'too': 96,\n",
       " 'could': 97,\n",
       " 'any': 98,\n",
       " 'movies': 99,\n",
       " 'after': 100,\n",
       " 'think': 101,\n",
       " 'characters': 102,\n",
       " 'watch': 103,\n",
       " 'two': 104,\n",
       " 'films': 105,\n",
       " 'character': 106,\n",
       " 'seen': 107,\n",
       " 'many': 108,\n",
       " 'being': 109,\n",
       " 'life': 110,\n",
       " 'plot': 111,\n",
       " 'never': 112,\n",
       " 'acting': 113,\n",
       " 'little': 114,\n",
       " 'best': 115,\n",
       " 'love': 116,\n",
       " 'over': 117,\n",
       " 'where': 118,\n",
       " 'did': 119,\n",
       " 'show': 120,\n",
       " 'know': 121,\n",
       " 'off': 122,\n",
       " 'ever': 123,\n",
       " 'does': 124,\n",
       " 'better': 125,\n",
       " 'your': 126,\n",
       " 'end': 127,\n",
       " 'still': 128,\n",
       " 'man': 129,\n",
       " 'here': 130,\n",
       " 'these': 131,\n",
       " 'say': 132,\n",
       " 'scene': 133,\n",
       " 'while': 134,\n",
       " 'why': 135,\n",
       " 'scenes': 136,\n",
       " 'go': 137,\n",
       " 'such': 138,\n",
       " 'something': 139,\n",
       " 'through': 140,\n",
       " 'should': 141,\n",
       " 'back': 142,\n",
       " \"i'm\": 143,\n",
       " 'real': 144,\n",
       " 'those': 145,\n",
       " 'watching': 146,\n",
       " 'now': 147,\n",
       " 'though': 148,\n",
       " \"doesn't\": 149,\n",
       " 'years': 150,\n",
       " 'old': 151,\n",
       " 'thing': 152,\n",
       " 'actors': 153,\n",
       " 'work': 154,\n",
       " '10': 155,\n",
       " 'before': 156,\n",
       " 'another': 157,\n",
       " \"didn't\": 158,\n",
       " 'new': 159,\n",
       " 'funny': 160,\n",
       " 'nothing': 161,\n",
       " 'actually': 162,\n",
       " 'makes': 163,\n",
       " 'director': 164,\n",
       " 'look': 165,\n",
       " 'find': 166,\n",
       " 'going': 167,\n",
       " 'few': 168,\n",
       " 'same': 169,\n",
       " 'part': 170,\n",
       " 'again': 171,\n",
       " 'every': 172,\n",
       " 'lot': 173,\n",
       " 'cast': 174,\n",
       " 'us': 175,\n",
       " 'quite': 176,\n",
       " 'down': 177,\n",
       " 'want': 178,\n",
       " 'world': 179,\n",
       " 'things': 180,\n",
       " 'pretty': 181,\n",
       " 'young': 182,\n",
       " 'seems': 183,\n",
       " 'around': 184,\n",
       " 'got': 185,\n",
       " 'horror': 186,\n",
       " 'however': 187,\n",
       " \"can't\": 188,\n",
       " 'fact': 189,\n",
       " 'take': 190,\n",
       " 'big': 191,\n",
       " 'enough': 192,\n",
       " 'long': 193,\n",
       " 'thought': 194,\n",
       " \"that's\": 195,\n",
       " 'both': 196,\n",
       " 'between': 197,\n",
       " 'series': 198,\n",
       " 'give': 199,\n",
       " 'may': 200,\n",
       " 'original': 201,\n",
       " 'own': 202,\n",
       " 'action': 203,\n",
       " \"i've\": 204,\n",
       " 'right': 205,\n",
       " 'without': 206,\n",
       " 'always': 207,\n",
       " 'times': 208,\n",
       " 'comedy': 209,\n",
       " 'point': 210,\n",
       " 'gets': 211,\n",
       " 'must': 212,\n",
       " 'come': 213,\n",
       " 'role': 214,\n",
       " \"isn't\": 215,\n",
       " 'saw': 216,\n",
       " 'almost': 217,\n",
       " 'interesting': 218,\n",
       " 'least': 219,\n",
       " 'family': 220,\n",
       " 'done': 221,\n",
       " \"there's\": 222,\n",
       " 'whole': 223,\n",
       " 'bit': 224,\n",
       " 'music': 225,\n",
       " 'script': 226,\n",
       " 'far': 227,\n",
       " 'making': 228,\n",
       " 'guy': 229,\n",
       " 'anything': 230,\n",
       " 'minutes': 231,\n",
       " 'feel': 232,\n",
       " 'last': 233,\n",
       " 'since': 234,\n",
       " 'might': 235,\n",
       " 'performance': 236,\n",
       " \"he's\": 237,\n",
       " '2': 238,\n",
       " 'probably': 239,\n",
       " 'kind': 240,\n",
       " 'am': 241,\n",
       " 'away': 242,\n",
       " 'yet': 243,\n",
       " 'rather': 244,\n",
       " 'tv': 245,\n",
       " 'worst': 246,\n",
       " 'girl': 247,\n",
       " 'day': 248,\n",
       " 'sure': 249,\n",
       " 'fun': 250,\n",
       " 'hard': 251,\n",
       " 'woman': 252,\n",
       " 'played': 253,\n",
       " 'each': 254,\n",
       " 'found': 255,\n",
       " 'anyone': 256,\n",
       " 'having': 257,\n",
       " 'although': 258,\n",
       " 'especially': 259,\n",
       " 'our': 260,\n",
       " 'believe': 261,\n",
       " 'course': 262,\n",
       " 'comes': 263,\n",
       " 'looking': 264,\n",
       " 'screen': 265,\n",
       " 'trying': 266,\n",
       " 'set': 267,\n",
       " 'goes': 268,\n",
       " 'looks': 269,\n",
       " 'place': 270,\n",
       " 'book': 271,\n",
       " 'different': 272,\n",
       " 'put': 273,\n",
       " 'ending': 274,\n",
       " 'money': 275,\n",
       " 'maybe': 276,\n",
       " 'once': 277,\n",
       " 'sense': 278,\n",
       " 'reason': 279,\n",
       " 'true': 280,\n",
       " 'actor': 281,\n",
       " 'everything': 282,\n",
       " \"wasn't\": 283,\n",
       " 'shows': 284,\n",
       " 'dvd': 285,\n",
       " 'three': 286,\n",
       " 'worth': 287,\n",
       " 'year': 288,\n",
       " 'job': 289,\n",
       " 'main': 290,\n",
       " 'someone': 291,\n",
       " 'together': 292,\n",
       " 'watched': 293,\n",
       " 'play': 294,\n",
       " 'american': 295,\n",
       " 'plays': 296,\n",
       " '1': 297,\n",
       " 'said': 298,\n",
       " 'effects': 299,\n",
       " 'later': 300,\n",
       " 'takes': 301,\n",
       " 'instead': 302,\n",
       " 'seem': 303,\n",
       " 'beautiful': 304,\n",
       " 'john': 305,\n",
       " 'himself': 306,\n",
       " 'version': 307,\n",
       " 'audience': 308,\n",
       " 'high': 309,\n",
       " 'house': 310,\n",
       " 'night': 311,\n",
       " 'during': 312,\n",
       " 'everyone': 313,\n",
       " 'left': 314,\n",
       " 'special': 315,\n",
       " 'seeing': 316,\n",
       " 'half': 317,\n",
       " 'excellent': 318,\n",
       " 'wife': 319,\n",
       " 'star': 320,\n",
       " 'shot': 321,\n",
       " 'war': 322,\n",
       " 'idea': 323,\n",
       " 'nice': 324,\n",
       " 'black': 325,\n",
       " 'less': 326,\n",
       " 'mind': 327,\n",
       " 'simply': 328,\n",
       " 'read': 329,\n",
       " 'second': 330,\n",
       " 'else': 331,\n",
       " \"you're\": 332,\n",
       " 'father': 333,\n",
       " 'fan': 334,\n",
       " 'poor': 335,\n",
       " 'help': 336,\n",
       " 'completely': 337,\n",
       " 'death': 338,\n",
       " '3': 339,\n",
       " 'used': 340,\n",
       " 'home': 341,\n",
       " 'either': 342,\n",
       " 'short': 343,\n",
       " 'line': 344,\n",
       " 'given': 345,\n",
       " 'men': 346,\n",
       " 'top': 347,\n",
       " 'dead': 348,\n",
       " 'budget': 349,\n",
       " 'try': 350,\n",
       " 'performances': 351,\n",
       " 'wrong': 352,\n",
       " 'classic': 353,\n",
       " 'boring': 354,\n",
       " 'enjoy': 355,\n",
       " 'need': 356,\n",
       " 'rest': 357,\n",
       " 'use': 358,\n",
       " 'kids': 359,\n",
       " 'hollywood': 360,\n",
       " 'low': 361,\n",
       " 'production': 362,\n",
       " 'until': 363,\n",
       " 'along': 364,\n",
       " 'full': 365,\n",
       " 'friends': 366,\n",
       " 'camera': 367,\n",
       " 'truly': 368,\n",
       " 'women': 369,\n",
       " 'awful': 370,\n",
       " 'video': 371,\n",
       " 'next': 372,\n",
       " 'tell': 373,\n",
       " 'remember': 374,\n",
       " 'couple': 375,\n",
       " 'stupid': 376,\n",
       " 'start': 377,\n",
       " 'stars': 378,\n",
       " 'perhaps': 379,\n",
       " 'sex': 380,\n",
       " 'mean': 381,\n",
       " 'came': 382,\n",
       " 'recommend': 383,\n",
       " 'let': 384,\n",
       " 'moments': 385,\n",
       " 'wonderful': 386,\n",
       " 'episode': 387,\n",
       " 'understand': 388,\n",
       " 'small': 389,\n",
       " 'face': 390,\n",
       " 'terrible': 391,\n",
       " 'playing': 392,\n",
       " 'school': 393,\n",
       " 'getting': 394,\n",
       " 'written': 395,\n",
       " 'doing': 396,\n",
       " 'often': 397,\n",
       " 'keep': 398,\n",
       " 'early': 399,\n",
       " 'name': 400,\n",
       " 'perfect': 401,\n",
       " 'style': 402,\n",
       " 'human': 403,\n",
       " 'definitely': 404,\n",
       " 'gives': 405,\n",
       " 'others': 406,\n",
       " 'itself': 407,\n",
       " 'lines': 408,\n",
       " 'live': 409,\n",
       " 'become': 410,\n",
       " 'dialogue': 411,\n",
       " 'person': 412,\n",
       " 'lost': 413,\n",
       " 'finally': 414,\n",
       " 'piece': 415,\n",
       " 'head': 416,\n",
       " 'case': 417,\n",
       " 'felt': 418,\n",
       " 'yes': 419,\n",
       " 'liked': 420,\n",
       " 'supposed': 421,\n",
       " 'title': 422,\n",
       " \"couldn't\": 423,\n",
       " 'absolutely': 424,\n",
       " 'white': 425,\n",
       " 'against': 426,\n",
       " 'boy': 427,\n",
       " 'picture': 428,\n",
       " 'sort': 429,\n",
       " 'worse': 430,\n",
       " 'certainly': 431,\n",
       " 'went': 432,\n",
       " 'entire': 433,\n",
       " 'waste': 434,\n",
       " 'cinema': 435,\n",
       " 'problem': 436,\n",
       " 'hope': 437,\n",
       " 'entertaining': 438,\n",
       " \"she's\": 439,\n",
       " 'mr': 440,\n",
       " 'overall': 441,\n",
       " 'evil': 442,\n",
       " 'called': 443,\n",
       " 'loved': 444,\n",
       " 'based': 445,\n",
       " 'oh': 446,\n",
       " 'several': 447,\n",
       " 'fans': 448,\n",
       " 'mother': 449,\n",
       " 'drama': 450,\n",
       " 'beginning': 451,\n",
       " 'killer': 452,\n",
       " 'lives': 453,\n",
       " '5': 454,\n",
       " 'direction': 455,\n",
       " 'care': 456,\n",
       " 'already': 457,\n",
       " 'becomes': 458,\n",
       " 'laugh': 459,\n",
       " 'example': 460,\n",
       " 'friend': 461,\n",
       " 'dark': 462,\n",
       " 'despite': 463,\n",
       " 'under': 464,\n",
       " 'seemed': 465,\n",
       " 'throughout': 466,\n",
       " '4': 467,\n",
       " 'turn': 468,\n",
       " 'unfortunately': 469,\n",
       " 'wanted': 470,\n",
       " \"i'd\": 471,\n",
       " '\\x96': 472,\n",
       " 'children': 473,\n",
       " 'final': 474,\n",
       " 'fine': 475,\n",
       " 'history': 476,\n",
       " 'amazing': 477,\n",
       " 'sound': 478,\n",
       " 'guess': 479,\n",
       " 'heart': 480,\n",
       " 'totally': 481,\n",
       " 'lead': 482,\n",
       " 'humor': 483,\n",
       " 'writing': 484,\n",
       " 'michael': 485,\n",
       " 'quality': 486,\n",
       " \"you'll\": 487,\n",
       " 'close': 488,\n",
       " 'son': 489,\n",
       " 'guys': 490,\n",
       " 'wants': 491,\n",
       " 'works': 492,\n",
       " 'behind': 493,\n",
       " 'tries': 494,\n",
       " 'art': 495,\n",
       " 'side': 496,\n",
       " 'game': 497,\n",
       " 'past': 498,\n",
       " 'able': 499,\n",
       " 'b': 500,\n",
       " 'days': 501,\n",
       " 'turns': 502,\n",
       " 'child': 503,\n",
       " \"they're\": 504,\n",
       " 'hand': 505,\n",
       " 'flick': 506,\n",
       " 'enjoyed': 507,\n",
       " 'act': 508,\n",
       " 'genre': 509,\n",
       " 'town': 510,\n",
       " 'favorite': 511,\n",
       " 'soon': 512,\n",
       " 'kill': 513,\n",
       " 'starts': 514,\n",
       " 'sometimes': 515,\n",
       " 'car': 516,\n",
       " 'gave': 517,\n",
       " 'run': 518,\n",
       " 'late': 519,\n",
       " 'eyes': 520,\n",
       " 'actress': 521,\n",
       " 'etc': 522,\n",
       " 'directed': 523,\n",
       " 'horrible': 524,\n",
       " \"won't\": 525,\n",
       " 'viewer': 526,\n",
       " 'brilliant': 527,\n",
       " 'parts': 528,\n",
       " 'self': 529,\n",
       " 'themselves': 530,\n",
       " 'hour': 531,\n",
       " 'expect': 532,\n",
       " 'thinking': 533,\n",
       " 'stories': 534,\n",
       " 'stuff': 535,\n",
       " 'girls': 536,\n",
       " 'obviously': 537,\n",
       " 'blood': 538,\n",
       " 'decent': 539,\n",
       " 'city': 540,\n",
       " 'voice': 541,\n",
       " 'highly': 542,\n",
       " 'myself': 543,\n",
       " 'feeling': 544,\n",
       " 'fight': 545,\n",
       " 'except': 546,\n",
       " 'slow': 547,\n",
       " 'matter': 548,\n",
       " 'type': 549,\n",
       " 'anyway': 550,\n",
       " 'kid': 551,\n",
       " 'roles': 552,\n",
       " 'killed': 553,\n",
       " 'heard': 554,\n",
       " 'god': 555,\n",
       " 'age': 556,\n",
       " 'says': 557,\n",
       " 'moment': 558,\n",
       " 'took': 559,\n",
       " 'leave': 560,\n",
       " 'writer': 561,\n",
       " 'strong': 562,\n",
       " 'cannot': 563,\n",
       " 'violence': 564,\n",
       " 'police': 565,\n",
       " 'hit': 566,\n",
       " 'stop': 567,\n",
       " 'happens': 568,\n",
       " 'particularly': 569,\n",
       " 'known': 570,\n",
       " 'involved': 571,\n",
       " 'happened': 572,\n",
       " 'extremely': 573,\n",
       " 'daughter': 574,\n",
       " 'obvious': 575,\n",
       " 'told': 576,\n",
       " 'chance': 577,\n",
       " 'living': 578,\n",
       " 'coming': 579,\n",
       " 'lack': 580,\n",
       " 'alone': 581,\n",
       " 'experience': 582,\n",
       " \"wouldn't\": 583,\n",
       " 'including': 584,\n",
       " 'murder': 585,\n",
       " 'attempt': 586,\n",
       " 's': 587,\n",
       " 'please': 588,\n",
       " 'james': 589,\n",
       " 'happen': 590,\n",
       " 'wonder': 591,\n",
       " 'crap': 592,\n",
       " 'ago': 593,\n",
       " 'brother': 594,\n",
       " \"film's\": 595,\n",
       " 'gore': 596,\n",
       " 'none': 597,\n",
       " 'complete': 598,\n",
       " 'interest': 599,\n",
       " 'score': 600,\n",
       " 'group': 601,\n",
       " 'cut': 602,\n",
       " 'simple': 603,\n",
       " 'save': 604,\n",
       " 'ok': 605,\n",
       " 'hell': 606,\n",
       " 'looked': 607,\n",
       " 'career': 608,\n",
       " 'number': 609,\n",
       " 'song': 610,\n",
       " 'possible': 611,\n",
       " 'seriously': 612,\n",
       " 'annoying': 613,\n",
       " 'shown': 614,\n",
       " 'exactly': 615,\n",
       " 'sad': 616,\n",
       " 'running': 617,\n",
       " 'musical': 618,\n",
       " 'serious': 619,\n",
       " 'taken': 620,\n",
       " 'yourself': 621,\n",
       " 'whose': 622,\n",
       " 'released': 623,\n",
       " 'cinematography': 624,\n",
       " 'david': 625,\n",
       " 'scary': 626,\n",
       " 'ends': 627,\n",
       " 'english': 628,\n",
       " 'hero': 629,\n",
       " 'usually': 630,\n",
       " 'hours': 631,\n",
       " 'reality': 632,\n",
       " 'opening': 633,\n",
       " \"i'll\": 634,\n",
       " 'across': 635,\n",
       " 'today': 636,\n",
       " 'jokes': 637,\n",
       " 'light': 638,\n",
       " 'hilarious': 639,\n",
       " 'somewhat': 640,\n",
       " 'usual': 641,\n",
       " 'started': 642,\n",
       " 'cool': 643,\n",
       " 'ridiculous': 644,\n",
       " 'body': 645,\n",
       " 'relationship': 646,\n",
       " 'view': 647,\n",
       " 'level': 648,\n",
       " 'opinion': 649,\n",
       " 'change': 650,\n",
       " 'happy': 651,\n",
       " 'middle': 652,\n",
       " 'taking': 653,\n",
       " 'wish': 654,\n",
       " 'husband': 655,\n",
       " 'finds': 656,\n",
       " 'saying': 657,\n",
       " 'order': 658,\n",
       " 'talking': 659,\n",
       " 'ones': 660,\n",
       " 'documentary': 661,\n",
       " 'shots': 662,\n",
       " 'huge': 663,\n",
       " 'novel': 664,\n",
       " 'female': 665,\n",
       " 'mostly': 666,\n",
       " 'robert': 667,\n",
       " 'power': 668,\n",
       " 'episodes': 669,\n",
       " 'room': 670,\n",
       " 'important': 671,\n",
       " 'rating': 672,\n",
       " 'talent': 673,\n",
       " 'five': 674,\n",
       " 'major': 675,\n",
       " 'turned': 676,\n",
       " 'strange': 677,\n",
       " 'word': 678,\n",
       " 'modern': 679,\n",
       " 'call': 680,\n",
       " 'apparently': 681,\n",
       " 'disappointed': 682,\n",
       " 'single': 683,\n",
       " 'events': 684,\n",
       " 'due': 685,\n",
       " 'four': 686,\n",
       " 'songs': 687,\n",
       " 'basically': 688,\n",
       " 'attention': 689,\n",
       " '7': 690,\n",
       " 'knows': 691,\n",
       " 'clearly': 692,\n",
       " 'supporting': 693,\n",
       " 'knew': 694,\n",
       " 'british': 695,\n",
       " 'television': 696,\n",
       " 'comic': 697,\n",
       " 'non': 698,\n",
       " 'fast': 699,\n",
       " 'earth': 700,\n",
       " 'country': 701,\n",
       " 'future': 702,\n",
       " 'cheap': 703,\n",
       " 'class': 704,\n",
       " 'thriller': 705,\n",
       " '8': 706,\n",
       " 'silly': 707,\n",
       " 'king': 708,\n",
       " 'problems': 709,\n",
       " \"aren't\": 710,\n",
       " 'easily': 711,\n",
       " 'words': 712,\n",
       " 'tells': 713,\n",
       " 'miss': 714,\n",
       " 'jack': 715,\n",
       " 'local': 716,\n",
       " 'sequence': 717,\n",
       " 'bring': 718,\n",
       " 'entertainment': 719,\n",
       " 'paul': 720,\n",
       " 'beyond': 721,\n",
       " 'upon': 722,\n",
       " 'whether': 723,\n",
       " 'predictable': 724,\n",
       " 'moving': 725,\n",
       " 'similar': 726,\n",
       " 'straight': 727,\n",
       " 'romantic': 728,\n",
       " 'sets': 729,\n",
       " 'review': 730,\n",
       " 'falls': 731,\n",
       " 'oscar': 732,\n",
       " 'mystery': 733,\n",
       " 'enjoyable': 734,\n",
       " 'needs': 735,\n",
       " 'appears': 736,\n",
       " 'talk': 737,\n",
       " 'rock': 738,\n",
       " 'george': 739,\n",
       " 'giving': 740,\n",
       " 'eye': 741,\n",
       " 'richard': 742,\n",
       " 'within': 743,\n",
       " 'ten': 744,\n",
       " 'animation': 745,\n",
       " 'message': 746,\n",
       " 'theater': 747,\n",
       " 'near': 748,\n",
       " 'above': 749,\n",
       " 'dull': 750,\n",
       " 'nearly': 751,\n",
       " 'sequel': 752,\n",
       " 'theme': 753,\n",
       " 'points': 754,\n",
       " \"'\": 755,\n",
       " 'stand': 756,\n",
       " 'mention': 757,\n",
       " 'lady': 758,\n",
       " 'bunch': 759,\n",
       " 'add': 760,\n",
       " 'feels': 761,\n",
       " 'herself': 762,\n",
       " 'release': 763,\n",
       " 'red': 764,\n",
       " 'team': 765,\n",
       " 'storyline': 766,\n",
       " 'surprised': 767,\n",
       " 'ways': 768,\n",
       " 'using': 769,\n",
       " 'named': 770,\n",
       " \"haven't\": 771,\n",
       " 'lots': 772,\n",
       " 'easy': 773,\n",
       " 'fantastic': 774,\n",
       " 'begins': 775,\n",
       " 'actual': 776,\n",
       " 'working': 777,\n",
       " 'effort': 778,\n",
       " 'york': 779,\n",
       " 'die': 780,\n",
       " 'hate': 781,\n",
       " 'french': 782,\n",
       " 'minute': 783,\n",
       " 'tale': 784,\n",
       " 'clear': 785,\n",
       " 'stay': 786,\n",
       " '9': 787,\n",
       " 'elements': 788,\n",
       " 'feature': 789,\n",
       " 'among': 790,\n",
       " 'follow': 791,\n",
       " 'comments': 792,\n",
       " 're': 793,\n",
       " 'viewers': 794,\n",
       " 'avoid': 795,\n",
       " 'sister': 796,\n",
       " 'showing': 797,\n",
       " 'typical': 798,\n",
       " 'editing': 799,\n",
       " \"what's\": 800,\n",
       " 'famous': 801,\n",
       " 'tried': 802,\n",
       " 'sorry': 803,\n",
       " 'dialog': 804,\n",
       " 'check': 805,\n",
       " 'fall': 806,\n",
       " 'period': 807,\n",
       " 'season': 808,\n",
       " 'form': 809,\n",
       " 'certain': 810,\n",
       " 'filmed': 811,\n",
       " 'weak': 812,\n",
       " 'soundtrack': 813,\n",
       " 'means': 814,\n",
       " 'buy': 815,\n",
       " 'material': 816,\n",
       " 'somehow': 817,\n",
       " 'realistic': 818,\n",
       " 'figure': 819,\n",
       " 'crime': 820,\n",
       " 'doubt': 821,\n",
       " 'gone': 822,\n",
       " 'peter': 823,\n",
       " 'tom': 824,\n",
       " 'kept': 825,\n",
       " 'viewing': 826,\n",
       " 't': 827,\n",
       " 'general': 828,\n",
       " 'leads': 829,\n",
       " 'greatest': 830,\n",
       " 'space': 831,\n",
       " 'lame': 832,\n",
       " 'suspense': 833,\n",
       " 'dance': 834,\n",
       " 'imagine': 835,\n",
       " 'brought': 836,\n",
       " 'third': 837,\n",
       " 'atmosphere': 838,\n",
       " 'hear': 839,\n",
       " 'particular': 840,\n",
       " 'sequences': 841,\n",
       " 'whatever': 842,\n",
       " 'parents': 843,\n",
       " 'move': 844,\n",
       " 'lee': 845,\n",
       " 'indeed': 846,\n",
       " 'learn': 847,\n",
       " 'rent': 848,\n",
       " 'de': 849,\n",
       " 'eventually': 850,\n",
       " 'note': 851,\n",
       " 'deal': 852,\n",
       " 'average': 853,\n",
       " 'reviews': 854,\n",
       " 'wait': 855,\n",
       " 'forget': 856,\n",
       " 'japanese': 857,\n",
       " 'sexual': 858,\n",
       " 'poorly': 859,\n",
       " 'premise': 860,\n",
       " 'okay': 861,\n",
       " 'zombie': 862,\n",
       " 'surprise': 863,\n",
       " 'believable': 864,\n",
       " 'stage': 865,\n",
       " 'possibly': 866,\n",
       " 'sit': 867,\n",
       " \"who's\": 868,\n",
       " 'decided': 869,\n",
       " 'expected': 870,\n",
       " \"you've\": 871,\n",
       " 'subject': 872,\n",
       " 'nature': 873,\n",
       " 'became': 874,\n",
       " 'difficult': 875,\n",
       " 'free': 876,\n",
       " 'killing': 877,\n",
       " 'screenplay': 878,\n",
       " 'truth': 879,\n",
       " 'romance': 880,\n",
       " 'dr': 881,\n",
       " 'nor': 882,\n",
       " 'reading': 883,\n",
       " 'needed': 884,\n",
       " 'question': 885,\n",
       " 'leaves': 886,\n",
       " 'street': 887,\n",
       " '20': 888,\n",
       " 'meets': 889,\n",
       " 'hot': 890,\n",
       " 'unless': 891,\n",
       " 'begin': 892,\n",
       " 'baby': 893,\n",
       " 'superb': 894,\n",
       " 'credits': 895,\n",
       " 'imdb': 896,\n",
       " 'otherwise': 897,\n",
       " 'write': 898,\n",
       " 'shame': 899,\n",
       " \"let's\": 900,\n",
       " 'situation': 901,\n",
       " 'dramatic': 902,\n",
       " 'memorable': 903,\n",
       " 'directors': 904,\n",
       " 'earlier': 905,\n",
       " 'meet': 906,\n",
       " 'disney': 907,\n",
       " 'open': 908,\n",
       " 'dog': 909,\n",
       " 'badly': 910,\n",
       " 'joe': 911,\n",
       " 'male': 912,\n",
       " 'weird': 913,\n",
       " 'acted': 914,\n",
       " 'forced': 915,\n",
       " 'laughs': 916,\n",
       " 'sci': 917,\n",
       " 'emotional': 918,\n",
       " 'older': 919,\n",
       " 'realize': 920,\n",
       " 'fi': 921,\n",
       " 'dream': 922,\n",
       " 'society': 923,\n",
       " 'writers': 924,\n",
       " 'interested': 925,\n",
       " 'footage': 926,\n",
       " 'forward': 927,\n",
       " 'comment': 928,\n",
       " 'crazy': 929,\n",
       " 'deep': 930,\n",
       " 'sounds': 931,\n",
       " 'plus': 932,\n",
       " 'beauty': 933,\n",
       " 'whom': 934,\n",
       " 'america': 935,\n",
       " 'fantasy': 936,\n",
       " 'directing': 937,\n",
       " 'keeps': 938,\n",
       " 'ask': 939,\n",
       " 'development': 940,\n",
       " 'features': 941,\n",
       " 'air': 942,\n",
       " 'quickly': 943,\n",
       " 'mess': 944,\n",
       " 'creepy': 945,\n",
       " 'towards': 946,\n",
       " 'perfectly': 947,\n",
       " 'mark': 948,\n",
       " 'worked': 949,\n",
       " 'box': 950,\n",
       " 'cheesy': 951,\n",
       " 'unique': 952,\n",
       " 'setting': 953,\n",
       " 'hands': 954,\n",
       " 'plenty': 955,\n",
       " 'result': 956,\n",
       " 'previous': 957,\n",
       " 'brings': 958,\n",
       " 'effect': 959,\n",
       " 'e': 960,\n",
       " 'total': 961,\n",
       " 'personal': 962,\n",
       " 'incredibly': 963,\n",
       " 'rate': 964,\n",
       " 'fire': 965,\n",
       " 'monster': 966,\n",
       " 'business': 967,\n",
       " 'leading': 968,\n",
       " 'apart': 969,\n",
       " 'casting': 970,\n",
       " 'admit': 971,\n",
       " 'joke': 972,\n",
       " 'powerful': 973,\n",
       " 'appear': 974,\n",
       " 'background': 975,\n",
       " 'telling': 976,\n",
       " 'girlfriend': 977,\n",
       " 'meant': 978,\n",
       " 'christmas': 979,\n",
       " 'hardly': 980,\n",
       " 'present': 981,\n",
       " 'battle': 982,\n",
       " 'potential': 983,\n",
       " 'create': 984,\n",
       " 'bill': 985,\n",
       " 'break': 986,\n",
       " 'pay': 987,\n",
       " 'masterpiece': 988,\n",
       " 'gay': 989,\n",
       " 'political': 990,\n",
       " 'return': 991,\n",
       " 'dumb': 992,\n",
       " 'fails': 993,\n",
       " 'fighting': 994,\n",
       " 'various': 995,\n",
       " 'era': 996,\n",
       " 'portrayed': 997,\n",
       " 'co': 998,\n",
       " 'cop': 999,\n",
       " 'secret': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------------------------\n",
    "# Some variable\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "num_tokens = vocabulary_size + 2\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Let truncate our word index\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "word_index = imdb.get_word_index()\n",
    "sorted_word_index = sorted(word_index.items(), key=lambda x: x[1])\n",
    "sorted_word_index = dict(sorted_word_index[:num_tokens])\n",
    "sorted_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.get(\"to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 9774 words (228 misses)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------------------------\n",
    "# Now, let's prepare a corresponding embedding matrix that we can use in a Keras Embedding layer.\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "hits = 0\n",
    "misses = 0\n",
    "for word, i in sorted_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None and len(embedding_vector) != 0:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i-1] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Next, we load the pre-trained word embeddings matrix into an Embedding layer.\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    trainable=False,\n",
    ")\n",
    "embedding_layer.build((1,))\n",
    "embedding_layer.set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MODEL 5 WITH GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_6 (Embedding)     (None, None, 300)         3000600   \n",
      "                                                                 \n",
      " conv1d_15 (Conv1D)          (None, None, 32)          28832     \n",
      "                                                                 \n",
      " max_pooling1d_15 (MaxPoolin  (None, None, 32)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " bidirectional_12 (Bidirecti  (None, 200)              106400    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,136,033\n",
      "Trainable params: 135,433\n",
      "Non-trainable params: 3,000,600\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 01:59:29.068290: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-05 01:59:29.068988: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-05 01:59:29.069872: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-05 01:59:29.132708: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-04-05 01:59:29.151319: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-05 01:59:29.151824: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-05 01:59:29.152376: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Building the model\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(filters=32, kernel_size=3, padding='same', activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "x = layers.Bidirectional(layers.LSTM(100))(x)\n",
    "preds = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 01:59:30.245228: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-05 01:59:30.246374: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-05 01:59:30.247212: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-05 01:59:30.329370: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-04-05 01:59:30.355897: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-05 01:59:30.356601: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-05 01:59:30.357358: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-05 01:59:30.608876: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-04-05 01:59:30.839688: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-05 01:59:30.840861: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-05 01:59:30.841527: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-05 01:59:30.922600: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-04-05 01:59:30.949715: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-05 01:59:30.950459: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-05 01:59:30.951069: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-05 01:59:31.214572: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 66s 164ms/step - loss: 0.6288 - accuracy: 0.6321\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 63s 161ms/step - loss: 0.4895 - accuracy: 0.7629\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 63s 160ms/step - loss: 0.3924 - accuracy: 0.8260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 02:02:41.995252: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-05 02:02:41.996053: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-05 02:02:41.997164: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-05 02:02:42.073984: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2024-04-05 02:02:42.100573: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-05 02:02:42.101338: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-05 02:02:42.102038: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.54%\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
